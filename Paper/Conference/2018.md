# NeurIPS

## Quantization
- BinGAN: Learning Compact Binary Descriptors with a Regularized GAN
- HitNet: Hybrid Ternary Recurrent Neural Network

## Pruning
- Frequency-Domain Dynamic Pruning for Convolutional Neural Networks
- Discrimination-aware Channel Pruning for Deep Neural Networks
- ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions

## Distillation
- Paraphrasing Complex Network: Network Compression via Factor Transfer

# ICML
## Compression
- Compressing Neural Networks using the Variational Information Bottleneck
- Weightless: Lossy weight encoding for deep neural network compression

# CVPR

## Quantization
- Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
- SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks
- Two-Step Quantization for Low-Bit Neural Networks
- CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization
- Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation
- Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks

## Pruning
- PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning
- "Learning-Compression" Algorithms for Neural Net Pruning
- NISP: Pruning Networks Using Neuron Importance Score Propagation
- Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition

## Low-Rank Approximation
- Wide Compression: Tensor Ring Nets

# ICLR

## Quantization
- Loss-aware Weight Quantization of Deep Networks
- Alternating Multi-bit Quantization for Recurrent Neural Networks
- Adaptive Quantization of Neural Networks 
- Variational Network Quantization
- Model compression via distillation and quantization 

## Pruning
- Stochastic activation pruning for robust adversarial defense
- Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers
- N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning 

## System
- Espresso: Efficient Forward Propagation for Binary Deep Neural Networks 