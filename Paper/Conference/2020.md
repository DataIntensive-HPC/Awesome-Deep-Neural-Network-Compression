# CVPR
- GAN Compression: Efficient Architectures for Interactive Conditional GANs
- Structured Multi-Hashing for Model Compression


## Quantization
- Structured Compression by Weight Encryption for Unstructured Pruning and Quantization
- Training Quantized Neural Networks With a Full-Precision Auxiliary Module
- Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-based Approach
- Adaptive Loss-aware Quantization for Multi-bit Networks
- ZeroQ: A Novel Zero Shot Quantization Framework
- BiDet: An Efficient Binarized Object Detector
- Forward and Backward Information Retention for Accurate Binary Neural Networks
- Binarizing MobileNet via Evolution-Based Searching


## Pruning

### Structure Pruning
- Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression
- Neural Network Pruning with Residual-Connections and Limited-Data
- HRank: Filter Pruning using High-Rank Feature Map
- DMCP: Differentiable Markov Channel Pruning for Neural Networks
- Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration
- Discrete Model Compression with Resource Constraint for Deep Neural Networks


## Distillation
- Few Sample Knowledge Distillation for Efficient Network Compression
- The Knowledge Within: Methods for Data-Free Model Compression


## Low-Rank Approximation
- Low-rank Compression of Neural Nets: Learning the Rank of Each Layer

## NAS
- APQ: Joint Search for Network Architecture, Pruning and Quantization Policy


# ICLR
- Mixed Precision DNNs: All you need is a good parametrization


## Pruning
- Comparing Fine-tuning and Rewinding in Neural Network Pruning
- A Signal Propagation Perspective for Pruning Neural Networks at Initialization 
- Data-Independent Neural Pruning via Coresets
- One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation 
- Lookahead: A Far-sighted Alternative of Magnitude-based Pruning
- Dynamic Model Pruning with Feedback 

### Structure Pruning
- Provable Filter Pruning for Efficient Neural Networks 

## Quantization
- Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware 
- AutoQ: Automated Kernel-Wise Neural Network Quantization 
- Additive Powers-of-Two Quantization: A Non-uniform Discretization for Neural Networks
- Learned Step Size Quantization
- Sampling-Free Learning of Bayesian Quantized Neural Networks
- Gradient $\ell_1$ Regularization for Quantization Robustness
- BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations 
- Training binary neural networks with real-to-binary convolutions 
- Critical initialisation in continuous approximations of binary neural networks 
- Mixed Precision DNNs: All you need is a good parametrization

## NAS
- In Search for a SAT-friendly Binarized Neural Network Architecture